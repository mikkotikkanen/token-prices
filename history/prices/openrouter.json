{
  "provider": "openrouter",
  "lastCrawled": "2026-01-12T11:41:51.215Z",
  "pricingUrl": "https://openrouter.ai/models",
  "changes": [
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "x-ai/grok-code-fast-1",
        "modelName": "xAI: Grok Code Fast 1",
        "inputPricePerMillion": 0.2,
        "outputPricePerMillion": 1.5,
        "contextWindow": 256000,
        "maxOutputTokens": 10000,
        "metadata": {
          "description": "Grok Code Fast 1 is a speedy and economical reasoning model that excels at agentic coding. With reasoning traces visible in the response, developers can steer Grok Code for high-quality work flows."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "anthropic/claude-sonnet-4.5",
        "modelName": "Anthropic: Claude Sonnet 4.5",
        "inputPricePerMillion": 3,
        "outputPricePerMillion": 15,
        "contextWindow": 1000000,
        "maxOutputTokens": 64000,
        "metadata": {
          "description": "Claude Sonnet 4.5 is Anthropic’s most advanced Sonnet model to date, optimized for real-world agents and coding workflows. It delivers state-of-the-art performance on coding benchmarks such as SWE-bench Verified, with improvements across system design, code security, and specification adherence. The model is designed for extended autonomous operation, maintaining task continuity across sessions and providing fact-based progress tracking.\n\nSonnet 4.5 also introduces stronger agentic capabilities, including improved tool orchestration, speculative parallel execution, and more efficient context and memory management. With enhanced context tracking and awareness of token usage across tool calls, it is particularly well-suited for multi-context and long-running workflows. Use cases span software engineering, cybersecurity, financial analysis, research agents, and other domains requiring sustained reasoning and tool use."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "x-ai/grok-4-fast",
        "modelName": "xAI: Grok 4 Fast",
        "inputPricePerMillion": 0.2,
        "outputPricePerMillion": 0.5,
        "contextWindow": 2000000,
        "maxOutputTokens": 30000,
        "metadata": {
          "description": "Grok 4 Fast is xAI's latest multimodal model with SOTA cost-efficiency and a 2M token context window. It comes in two flavors: non-reasoning and reasoning. Read more about the model on xAI's [news post](http://x.ai/news/grok-4-fast).\n\nReasoning can be enabled/disabled using the `reasoning` `enabled` parameter in the API. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#controlling-reasoning-tokens)"
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "google/gemini-2.5-pro",
        "modelName": "Google: Gemini 2.5 Pro",
        "inputPricePerMillion": 1.25,
        "outputPricePerMillion": 10,
        "contextWindow": 1048576,
        "maxOutputTokens": 65536,
        "metadata": {
          "description": "Gemini 2.5 Pro is Google’s state-of-the-art AI model designed for advanced reasoning, coding, mathematics, and scientific tasks. It employs “thinking” capabilities, enabling it to reason through responses with enhanced accuracy and nuanced context handling. Gemini 2.5 Pro achieves top-tier performance on multiple benchmarks, including first-place positioning on the LMArena leaderboard, reflecting superior human-preference alignment and complex problem-solving abilities."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "deepseek/deepseek-chat-v3-0324",
        "modelName": "DeepSeek: DeepSeek V3 0324",
        "inputPricePerMillion": 0.19,
        "outputPricePerMillion": 0.87,
        "contextWindow": 163840,
        "maxOutputTokens": 65536,
        "metadata": {
          "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.\n\nIt succeeds the [DeepSeek V3](/deepseek/deepseek-chat-v3) model and performs really well on a variety of tasks."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "openai/gpt-oss-120b",
        "modelName": "OpenAI: gpt-oss-120b",
        "inputPricePerMillion": 0.039,
        "outputPricePerMillion": 0.19,
        "contextWindow": 131072,
        "maxOutputTokens": null,
        "metadata": {
          "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "google/gemini-3-pro-preview",
        "modelName": "Google: Gemini 3 Pro Preview",
        "inputPricePerMillion": 2,
        "outputPricePerMillion": 12,
        "contextWindow": 1048576,
        "maxOutputTokens": 65536,
        "metadata": {
          "description": "Gemini 3 Pro is Google’s flagship frontier model for high-precision multimodal reasoning, combining strong performance across text, image, video, audio, and code with a 1M-token context window. Reasoning Details must be preserved when using multi-turn tool calling, see our docs here: https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks. It delivers state-of-the-art benchmark results in general reasoning, STEM problem solving, factual QA, and multimodal understanding, including leading scores on LMArena, GPQA Diamond, MathArena Apex, MMMU-Pro, and Video-MMMU. Interactions emphasize depth and interpretability: the model is designed to infer intent with minimal prompting and produce direct, insight-focused responses.\n\nBuilt for advanced development and agentic workflows, Gemini 3 Pro provides robust tool-calling, long-horizon planning stability, and strong zero-shot generation for complex UI, visualization, and coding tasks. It excels at agentic coding (SWE-Bench Verified, Terminal-Bench 2.0), multimodal analysis, and structured long-form tasks such as research synthesis, planning, and interactive learning experiences. Suitable applications include autonomous agents, coding assistants, multimodal analytics, scientific reasoning, and high-context information processing."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "google/gemini-2.5-flash-preview-09-2025",
        "modelName": "Google: Gemini 2.5 Flash Preview 09-2025",
        "inputPricePerMillion": 0.3,
        "outputPricePerMillion": 2.5,
        "contextWindow": 1048576,
        "maxOutputTokens": 65536,
        "metadata": {
          "description": "Gemini 2.5 Flash Preview September 2025 Checkpoint is Google's state-of-the-art workhorse model, specifically designed for advanced reasoning, coding, mathematics, and scientific tasks. It includes built-in \"thinking\" capabilities, enabling it to provide responses with greater accuracy and nuanced context handling. \n\nAdditionally, Gemini 2.5 Flash is configurable through the \"max tokens for reasoning\" parameter, as described in the documentation (https://openrouter.ai/docs/use-cases/reasoning-tokens#max-tokens-for-reasoning)."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "openai/gpt-4.1-mini",
        "modelName": "OpenAI: GPT-4.1 Mini",
        "inputPricePerMillion": 0.4,
        "outputPricePerMillion": 1.6,
        "contextWindow": 1047576,
        "maxOutputTokens": 32768,
        "metadata": {
          "description": "GPT-4.1 Mini is a mid-sized model delivering performance competitive with GPT-4o at substantially lower latency and cost. It retains a 1 million token context window and scores 45.1% on hard instruction evals, 35.8% on MultiChallenge, and 84.1% on IFEval. Mini also shows strong coding ability (e.g., 31.6% on Aider’s polyglot diff benchmark) and vision understanding, making it suitable for interactive applications with tight performance constraints."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "deepseek/deepseek-v3.2-exp",
        "modelName": "DeepSeek: DeepSeek V3.2 Exp",
        "inputPricePerMillion": 0.21,
        "outputPricePerMillion": 0.32,
        "contextWindow": 163840,
        "maxOutputTokens": 65536,
        "metadata": {
          "description": "DeepSeek-V3.2-Exp is an experimental large language model released by DeepSeek as an intermediate step between V3.1 and future architectures. It introduces DeepSeek Sparse Attention (DSA), a fine-grained sparse attention mechanism designed to improve training and inference efficiency in long-context scenarios while maintaining output quality. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model was trained under conditions aligned with V3.1-Terminus to enable direct comparison. Benchmarking shows performance roughly on par with V3.1 across reasoning, coding, and agentic tool-use tasks, with minor tradeoffs and gains depending on the domain. This release focuses on validating architectural optimizations for extended context lengths rather than advancing raw task accuracy, making it primarily a research-oriented model for exploring efficient transformer designs."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "google/gemini-2.5-flash-lite-preview-09-2025",
        "modelName": "Google: Gemini 2.5 Flash Lite Preview 09-2025",
        "inputPricePerMillion": 0.1,
        "outputPricePerMillion": 0.4,
        "contextWindow": 1048576,
        "maxOutputTokens": 65536,
        "metadata": {
          "description": "Gemini 2.5 Flash-Lite is a lightweight reasoning model in the Gemini 2.5 family, optimized for ultra-low latency and cost efficiency. It offers improved throughput, faster token generation, and better performance across common benchmarks compared to earlier Flash models. By default, \"thinking\" (i.e. multi-pass reasoning) is disabled to prioritize speed, but developers can enable it via the [Reasoning API parameter](https://openrouter.ai/docs/use-cases/reasoning-tokens) to selectively trade off cost for intelligence. "
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "qwen/qwen3-235b-a22b-2507",
        "modelName": "Qwen: Qwen3 235B A22B Instruct 2507",
        "inputPricePerMillion": 0.071,
        "outputPricePerMillion": 0.463,
        "contextWindow": 262144,
        "maxOutputTokens": null,
        "metadata": {
          "description": "Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (<think> blocks).\n\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "qwen/qwen3-coder-30b-a3b-instruct",
        "modelName": "Qwen: Qwen3 Coder 30B A3B Instruct",
        "inputPricePerMillion": 0.07,
        "outputPricePerMillion": 0.27,
        "contextWindow": 160000,
        "maxOutputTokens": 32768,
        "metadata": {
          "description": "Qwen3-Coder-30B-A3B-Instruct is a 30.5B parameter Mixture-of-Experts (MoE) model with 128 experts (8 active per forward pass), designed for advanced code generation, repository-scale understanding, and agentic tool use. Built on the Qwen3 architecture, it supports a native context length of 256K tokens (extendable to 1M with Yarn) and performs strongly in tasks involving function calls, browser use, and structured code completion.\n\nThis model is optimized for instruction-following without “thinking mode”, and integrates well with OpenAI-compatible tool-use formats. "
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "qwen/qwen3-vl-235b-a22b-instruct",
        "modelName": "Qwen: Qwen3 VL 235B A22B Instruct",
        "inputPricePerMillion": 0.2,
        "outputPricePerMillion": 1.2,
        "contextWindow": 262144,
        "maxOutputTokens": null,
        "metadata": {
          "description": "Qwen3-VL-235B-A22B Instruct is an open-weight multimodal model that unifies strong text generation with visual understanding across images and video. The Instruct model targets general vision-language use (VQA, document parsing, chart/table extraction, multilingual OCR). The series emphasizes robust perception (recognition of diverse real-world and synthetic categories), spatial understanding (2D/3D grounding), and long-form visual comprehension, with competitive results on public multimodal benchmarks for both perception and reasoning.\n\nBeyond analysis, Qwen3-VL supports agentic interaction and tool use: it can follow complex instructions over multi-image, multi-turn dialogues; align text to video timelines for precise temporal queries; and operate GUI elements for automation tasks. The models also enable visual coding workflows—turning sketches or mockups into code and assisting with UI debugging—while maintaining strong text-only performance comparable to the flagship Qwen3 language models. This makes Qwen3-VL suitable for production scenarios spanning document AI, multilingual OCR, software/UI assistance, spatial/embodied tasks, and research on vision-language agents."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "google/gemini-3-flash-preview",
        "modelName": "Google: Gemini 3 Flash Preview",
        "inputPricePerMillion": 0.5,
        "outputPricePerMillion": 3,
        "contextWindow": 1048576,
        "maxOutputTokens": 65535,
        "metadata": {
          "description": "Gemini 3 Flash Preview is a high speed, high value thinking model designed for agentic workflows, multi turn chat, and coding assistance. It delivers near Pro level reasoning and tool use performance with substantially lower latency than larger Gemini variants, making it well suited for interactive development, long running agent loops, and collaborative coding tasks. Compared to Gemini 2.5 Flash, it provides broad quality improvements across reasoning, multimodal understanding, and reliability.\n\nThe model supports a 1M token context window and multimodal inputs including text, images, audio, video, and PDFs, with text output. It includes configurable reasoning via thinking levels (minimal, low, medium, high), structured output, tool use, and automatic context caching. Gemini 3 Flash Preview is optimized for users who want strong reasoning and agentic behavior without the cost or latency of full scale frontier models."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "deepseek/deepseek-r1-0528-qwen3-8b",
        "modelName": "DeepSeek: DeepSeek R1 0528 Qwen3 8B",
        "inputPricePerMillion": 0.06,
        "outputPricePerMillion": 0.09,
        "contextWindow": 128000,
        "maxOutputTokens": 32000,
        "metadata": {
          "description": "DeepSeek-R1-0528 is a lightly upgraded release of DeepSeek R1 that taps more compute and smarter post-training tricks, pushing its reasoning and inference to the brink of flagship models like O3 and Gemini 2.5 Pro.\nIt now tops math, programming, and logic leaderboards, showcasing a step-change in depth-of-thought.\nThe distilled variant, DeepSeek-R1-0528-Qwen3-8B, transfers this chain-of-thought into an 8 B-parameter form, beating standard Qwen3 8B by +10 pp and tying the 235 B “thinking” giant on AIME 2024."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "openai/gpt-5-codex",
        "modelName": "OpenAI: GPT-5 Codex",
        "inputPricePerMillion": 1.25,
        "outputPricePerMillion": 10,
        "contextWindow": 400000,
        "maxOutputTokens": 128000,
        "metadata": {
          "description": "GPT-5-Codex is a specialized version of GPT-5 optimized for software engineering and coding workflows. It is designed for both interactive development sessions and long, independent execution of complex engineering tasks. The model supports building projects from scratch, feature development, debugging, large-scale refactoring, and code review. Compared to GPT-5, Codex is more steerable, adheres closely to developer instructions, and produces cleaner, higher-quality code outputs. Reasoning effort can be adjusted with the `reasoning.effort` parameter. Read the [docs here](https://openrouter.ai/docs/use-cases/reasoning-tokens#reasoning-effort-level)\n\nCodex integrates into developer environments including the CLI, IDE extensions, GitHub, and cloud tasks. It adapts reasoning effort dynamically—providing fast responses for small tasks while sustaining extended multi-hour runs for large projects. The model is trained to perform structured code reviews, catching critical flaws by reasoning over dependencies and validating behavior against tests. It also supports multimodal inputs such as images or screenshots for UI development and integrates tool use for search, dependency installation, and environment setup. Codex is intended specifically for agentic coding applications."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "minimax/minimax-m2.1",
        "modelName": "MiniMax: MiniMax M2.1",
        "inputPricePerMillion": 0.28,
        "outputPricePerMillion": 1.2,
        "contextWindow": 196608,
        "maxOutputTokens": null,
        "metadata": {
          "description": "MiniMax-M2.1 is a lightweight, state-of-the-art large language model optimized for coding, agentic workflows, and modern application development. With only 10 billion activated parameters, it delivers a major jump in real-world capability while maintaining exceptional latency, scalability, and cost efficiency.\n\nCompared to its predecessor, M2.1 delivers cleaner, more concise outputs and faster perceived response times. It shows leading multilingual coding performance across major systems and application languages, achieving 49.4% on Multi-SWE-Bench and 72.5% on SWE-Bench Multilingual, and serves as a versatile agent “brain” for IDEs, coding tools, and general-purpose assistance.\n\nTo avoid degrading this model's performance, MiniMax highly recommends preserving reasoning between turns. Learn more about using reasoning_details to pass back reasoning in our [docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks)."
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "deepseek/deepseek-v3.1-terminus",
        "modelName": "DeepSeek: DeepSeek V3.1 Terminus",
        "inputPricePerMillion": 0.21,
        "outputPricePerMillion": 0.79,
        "contextWindow": 163840,
        "maxOutputTokens": null,
        "metadata": {
          "description": "DeepSeek-V3.1 Terminus is an update to [DeepSeek V3.1](/deepseek/deepseek-chat-v3.1) that maintains the model's original capabilities while addressing issues reported by users, including language consistency and agent capabilities, further optimizing the model's performance in coding and search agents. It is a large hybrid reasoning model (671B parameters, 37B active) that supports both thinking and non-thinking modes. It extends the DeepSeek-V3 base with a two-phase long-context training process, reaching up to 128K tokens, and uses FP8 microscaling for efficient inference. Users can control the reasoning behaviour with the `reasoning` `enabled` boolean. [Learn more in our docs](https://openrouter.ai/docs/use-cases/reasoning-tokens#enable-reasoning-with-default-config)\n\nThe model improves tool use, code generation, and reasoning efficiency, achieving performance comparable to DeepSeek-R1 on difficult benchmarks while responding more quickly. It supports structured tool calling, code agents, and search agents, making it suitable for research, coding, and agentic workflows. "
        }
      }
    },
    {
      "date": "2026-01-12",
      "changeType": "added",
      "pricing": {
        "modelId": "openai/gpt-5-nano",
        "modelName": "OpenAI: GPT-5 Nano",
        "inputPricePerMillion": 0.05,
        "outputPricePerMillion": 0.4,
        "contextWindow": 400000,
        "maxOutputTokens": 128000,
        "metadata": {
          "description": "GPT-5-Nano is the smallest and fastest variant in the GPT-5 system, optimized for developer tools, rapid interactions, and ultra-low latency environments. While limited in reasoning depth compared to its larger counterparts, it retains key instruction-following and safety features. It is the successor to GPT-4.1-nano and offers a lightweight option for cost-sensitive or real-time applications."
        }
      }
    }
  ]
}